{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610664f4",
   "metadata": {},
   "source": [
    "# Hands-On Lab 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466373f2",
   "metadata": {},
   "source": [
    "In this lab, you will perform feature engineering and train a tuned *DecisionTreeClassifier* on the *Titanic* dataset. You will then compare the predictive performance of this lab's *DecisionTreeClassifier* to Lab #3's *DecisionTreeClassifier*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436627e",
   "metadata": {},
   "source": [
    "### Step 1 - Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26aebf",
   "metadata": {},
   "source": [
    "The *titantic_train.csv* file is provided along with the lab Jupyter Notebook file for you to download from the course page. Run the following code cell to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Titanic training data from CSV file\n",
    "titanic_train = pd.read_csv('titanic_train.csv')\n",
    "titanic_train.head(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec772d77-f3f3-4a88-b9f8-64620cb69873",
   "metadata": {},
   "source": [
    "### Step 2 - Engineer the *Female* Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5c623-97e9-4b78-8222-ea64c72ecb46",
   "metadata": {},
   "source": [
    "Currently, the *Sex* feature is encoded using the string values of *male* and *female*. As you saw in Lab #3, one-hot encoding the *Sex* feature produces two binary features - *Sex_female* and *Sex_male*. This is not optimal for machine learning as the *Sex* feature's information is spread across two new features. A better option is to create a binary *Female* feature instead. Type the following code into the blank code cell in your lab notebook and run it to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4105ad9a-6456-4130-89a4-94b38b30eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1626dec4-8b7a-4ae8-9e42-96940d74f7e8",
   "metadata": {},
   "source": [
    "### Step 3 - Engineer the *Party* Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c85ded-66c2-477b-ad11-31ad0f80e791",
   "metadata": {},
   "source": [
    "As you've learned in a previous lesson, the *Titanic* dataset offers opportunities to engineer features based on domain knowledge. Specically, engineering features to calculate the total number of family members traveling together and how much *Fare* they paid, on average, for each family member. Type the following code into the blank code cell in your lab notebook and run it to produce the results.\n",
    "\n",
    "NOTE - Be sure to copy and paste the code from Step 2 and extend it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420680a-65c5-4771-b470-85483b6ef135",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a8c07-bbec-4879-bc48-5020cb5f2823",
   "metadata": {},
   "source": [
    "### Step 4 - Impute Missing *Embarked* Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b683c-431c-4db2-b94c-5d32f1865a69",
   "metadata": {},
   "source": [
    "The simplest form of replacing missing values (i.e., *imputation*) is to use simple calculations like the mean (or average), median, and mode. If you're unfamiliar, the mode is the most frequencly occuring values. Running the following code outputs the value counts of the *Embarked* feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63157dc-4789-4364-8727-a139eb1127e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure to count missing values as well!\n",
    "train_wrangled['Embarked'].value_counts(dropna = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f06482-5f77-4d4e-905d-d686a3faa567",
   "metadata": {},
   "source": [
    "The output above shows that the most common *Embarked* value (or mode) is *S*. Using this knowledge you can *impute* the missing *Embarked* values to be *S*. The code below uses the [***fillna()* method**](https://pandas.pydata.org/docs/reference/api/pandas.Series.fillna.html) to replace missing *Embarked* values with *S*. Run the following code to see the output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10057fa4-de85-45e1-b67f-376a46c0dd05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train_wrangled DataFrame\n",
    "train_wrangled = (titanic_train\n",
    "                    .assign(Female = lambda df_: df_['Sex'].replace({'female': '1', 'male': '0'}).astype(int),\n",
    "                            PartySize = lambda df_: df_['SibSp'] + df_['Parch'] + 1,\n",
    "                            PartyFare = lambda df_: df_['Fare'] / df_['PartySize'],\n",
    "                            Embarked = lambda df_: df_['Embarked'].fillna('S'))\n",
    "                 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8dcbfca-93f0-4ce1-bd8c-439097aa638d",
   "metadata": {},
   "source": [
    "### Step 5 - Extract the *Title* Feature"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "792ccbef-6bc4-4154-ad5e-4ca5e3d3690e",
   "metadata": {},
   "source": [
    "As covered in a previous lesson, the *Name* feature includes the titles of passengers (e.g., *Master.*). Passenger titles appear to contain information regarding both the *Sex* and *Age* of the passenger. In this regard, titles could be a proxy feature for both *Sex* and *Age*. The following code creates the *Title* feature by using the [***str.split()* method**](https://pandas.pydata.org/docs/reference/api/pandas.Series.str.split.html) extracting data from the *Name* feature. Type the following code into the blank code cell in your lab notebook and run it to produce the results.\n",
    "\n",
    "NOTE - Be sure to copy and paste the code from Step 4 and extend it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c777f615-14c0-4c45-9b3f-659929ccf7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18803d3c-a857-4ee0-bb54-26e9da6b5187",
   "metadata": {},
   "source": [
    "### Step 6 - Drop Unneeded Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d73a065-28c5-4228-be65-779595ef1153",
   "metadata": {},
   "source": [
    "At this stage, there are a number of features that will not be used to train the model. For example, *PassengerId* and *CommaSplit* are poor features to use in a machine learning model. Type the following code into the blank code cell in your lab notebook and run it to produce the results.\n",
    "\n",
    "NOTE - Be sure to copy and paste the code from Step 5 and extend it below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea77ba5-eba3-49a0-b25a-f3baf8812b9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b70e83",
   "metadata": {},
   "source": [
    "### Step 7 - Encode Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6def3a98",
   "metadata": {},
   "source": [
    "For this lab, a few categorical features have to be one-hot encoded. Type the following code into the blank code cell in your lab notebook and run it to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d954b41d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc413f52",
   "metadata": {},
   "source": [
    "### Step 8 - Build Predictors DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9411a35",
   "metadata": {},
   "source": [
    "With the categorical features one-hot encoded in the *train_cat* DataFrame, it's time to create the predictor DataFrame that will be used in training the decision tree. Type the following code into the blank code cell in your lab notebook and run it to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a33783",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1ceb5",
   "metadata": {},
   "source": [
    "### Step 9 - Train a Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2dafc5",
   "metadata": {},
   "source": [
    "As the *Survived* label is numeric (i.e., 1 == Survived, 0 == Perished), it does not need to be encoded. The following code performs a grid search using *min_samples_leaf* and *min_impurity_decrease* hyperparameters. The best model is evaluated using *accuracy* as the measure of awesomeness. Type the following code into the blank code cell in your lab notebook and run it to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f9745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbef417",
   "metadata": {},
   "source": [
    "### Step 10 - Visualize the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9394c0b",
   "metadata": {},
   "source": [
    "When visualizing the *DecisionTreeClassifier* model, the labels of *Perished* and *Survived* will be used instead of ones and zeroes. The *best_estimator_* attribute of the *grid_cv* object provides a model trained on all the data using the best hyperparameter values. Type the following code into the blank code cell in your lab notebook and run it to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5e594f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce108c7d",
   "metadata": {},
   "source": [
    "### Step 11 - Evaluating Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f209d2df",
   "metadata": {},
   "source": [
    "The tuned *decision_tree* model above isn't particularly complex. While you could spend time pouring over all the paths through the tree, a far more efficient way to understand the effectiveness of the model's predictions is to get the bias and variance. Type the following code into the blank code cell in your lab notebook and run it to produce the results.\n",
    "\n",
    "Note the following in the output:\n",
    "* The model's mean accuracy (i.e., *bias*) is 82.4% across the 100 folds\n",
    "* However, the model's standard deviation (i.e., *variance*) is 4% across the 100 folds\n",
    "\n",
    "To interpret the variance:\n",
    "* We would expect the model to score between 78.4% and 86.4% accuracy about 2/3 of the time.\n",
    "* We would expect the model to score between 74.4% and 90.4% accuracy about 95% of the time.\n",
    "\n",
    "**Bottom Line -** This model's bias and variance have improved compared to the Lab 3 model! However, the variance remains high."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cedd74a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
