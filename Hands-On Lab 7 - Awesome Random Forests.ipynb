{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "610664f4",
   "metadata": {},
   "source": [
    "# Hands-On Lab 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466373f2",
   "metadata": {},
   "source": [
    "This lab is the capstone of the course, consisting of three parts:\n",
    "\n",
    "1. Evaluating features using *permutation importance*.\n",
    "2. Tuning a *RandomForestClassifier* for better predictive performance.\n",
    "3. Making predictions using the *RandomForestClassifier* for the *Titanic* test dataset.\n",
    "\n",
    "**NOTE -** The code in this lab assumes a top-to-bottom execution order. If things don't look right, just execute all the code cells again starting from the top."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95cc0e8e-4308-4fcd-b1f8-d1f240c113d2",
   "metadata": {},
   "source": [
    "## Part 1 - Evaluating Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e436627e",
   "metadata": {},
   "source": [
    "### Step 1 - Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c26aebf",
   "metadata": {},
   "source": [
    "The *titantic_train.csv* file is provided along with the lab Jupyter Notebook file for you to download from the course page. Run the following code cell to load the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a267cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load Titanic training data from CSV file\n",
    "titanic_train = pd.read_csv('titanic_train.csv')\n",
    "titanic_train.head(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec772d77-f3f3-4a88-b9f8-64620cb69873",
   "metadata": {},
   "source": [
    "### Step 2 - Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36f5c623-97e9-4b78-8222-ea64c72ecb46",
   "metadata": {},
   "source": [
    "The *RandomForestClassifier* model will leverage the features that you created in Lab #4. Run the following code to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4105ad9a-6456-4130-89a4-94b38b30eccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the train_wrangled DataFrame\n",
    "train_wrangled = (titanic_train\n",
    "                    .assign(Female = lambda df_: df_['Sex'].replace({'female': 1, 'male': 0}),\n",
    "                            PartySize = lambda df_: df_['SibSp'] + df_['Parch'] + 1,\n",
    "                            PartyFare = lambda df_: df_['Fare'] / df_['PartySize'],\n",
    "                            Embarked = lambda df_: df_['Embarked'].fillna('S'),\n",
    "                            CommaSplit = lambda df_: df_['Name'].str.split(', ', expand = True).loc[:, 1],\n",
    "                            Title = lambda df_: df_['CommaSplit'].str.split('.', expand = True).loc[:, 0])\n",
    "                 )\n",
    "\n",
    "train_wrangled.head(n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1626dec4-8b7a-4ae8-9e42-96940d74f7e8",
   "metadata": {},
   "source": [
    "### Step 3 - Encode Categorical Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c85ded-66c2-477b-ad11-31ad0f80e791",
   "metadata": {},
   "source": [
    "As you've seen in previous labs, the categorical features must be one-hot encoded. Run the following code cell to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f420680a-65c5-4771-b470-85483b6ef135",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Designate the categorical features to use\n",
    "cat_features = ['Pclass', 'Embarked', 'Title']\n",
    "\n",
    "# Instatiate a OneHotEncoder \n",
    "cat_encoder = OneHotEncoder(sparse_output = False)\n",
    "cat_encoder.set_output(transform = 'pandas')\n",
    "\n",
    "# Learn the encodings and transform data\n",
    "train_cat = cat_encoder.fit_transform(train_wrangled[cat_features])\n",
    "train_cat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191a8c07-bbec-4879-bc48-5020cb5f2823",
   "metadata": {},
   "source": [
    "### Step 4 - Build Predictors DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3b683c-431c-4db2-b94c-5d32f1865a69",
   "metadata": {},
   "source": [
    "The model will use both the *Female* and *Title* features to allow comparisons. Run the code cell below to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f63157dc-4789-4364-8727-a139eb1127e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Designate numeric features\n",
    "num_features = ['SibSp', 'Parch', 'Female', 'PartySize', 'PartyFare']\n",
    "\n",
    "# Build the predictors DataFrame\n",
    "titanic_X = pd.concat([train_wrangled[num_features], train_cat], axis = 1)\n",
    "titanic_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14d1ceb5",
   "metadata": {},
   "source": [
    "### Step 5 - Train a Mighty Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af2dafc5",
   "metadata": {},
   "source": [
    "The following code trains a *RandomForestClassifier* as you saw in Lab #6. The resulting model will be used with permutation importance to evaluate features. Notices that the OOB generalization estimate is slightly lower than what you saw in Lab #6. This is due to the addition of the *Female* feature. Run the code cell below to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49f9745",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Intstantiate the object, make sure OOB score is calculated\n",
    "titanic_rf = RandomForestClassifier(random_state = 12345, oob_score = True)\n",
    "\n",
    "# Train the random forest\n",
    "titanic_rf.fit(titanic_X, titanic_train['Survived'])\n",
    "\n",
    "# What is the accuracy estimate?\n",
    "print(f'Estimated accuracy with OOB data: {titanic_rf.oob_score_:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbef417",
   "metadata": {},
   "source": [
    "### Step 6 - Permutation Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9394c0b",
   "metadata": {},
   "source": [
    "The following code evaluates each of the features used to train the *RandomForestClassifier*. The *n_repeats* parameter is set to conduct 10 separate permutation importance tests where the results are averaged together. Higher repeats gives you better results, but takes longer to process. Type the following code into the blank code cell in your lab notebook and run it to produce the results.\n",
    "\n",
    "**NOTE -** The code below sets the *n_jobs* parameter to 2. You can increase this number to speed up the processing, but be sure to set this to a resonable number based on the number of your computer's cores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b27f10a-1c34-49cd-9f4d-9d8b8b4e1950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c71296-0cbc-44c9-a104-1969e180bfd1",
   "metadata": {},
   "source": [
    "### Step 7 - The Top Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3bd3642-3530-4aae-be62-30d8b8193d29",
   "metadata": {},
   "source": [
    "The following output shows the top features based on *permutation importance*. Notice how the *PartyFare* feature was found to be the most importance because, on average, the predictive accuracy of the model decreased by 12.1% when the values of *PartyFare* were permuted (i.e., randomized). Run the code cell below to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd90ea62-81e7-4c89-bad5-542761a22b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the top 15 features\n",
    "importance_df.head(n = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff957a95-ba21-4c35-9c29-881c0ee674ac",
   "metadata": {},
   "source": [
    "### Step 8 - The Bottom Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ce9197-2321-4f2a-9e30-23ecc0d5ec00",
   "metadata": {},
   "source": [
    "The following output shows the bottom features based on *permutation importance*. Notice how the *Title_Don* feature was found to be the least importance because accuracy decreased by 0% on average. Another way to think about this is that the *Title_Don* feature offers no predictive information at all! Run the code cell below to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc2256e2-9d58-440e-b210-e21c7d084c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the bottom 15 features\n",
    "importance_df.tail(n = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eafee89-5639-4f01-bd47-dfaa13398ba8",
   "metadata": {},
   "source": [
    "### Step 9 - *Title* Feature Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5695184c-4bba-40be-992e-885428158259",
   "metadata": {},
   "source": [
    "While there are many reasons why a feature might not offer any predictive information, one of the most common reasons for categorical features is that certain values are rare. These are sometimes referred to as \"long-tail values.\" The following output shows that the features found to be the least important are relatively rare. Run the code cell below to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df677e1-362d-43ad-99a4-4336d431ef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the counts of each Title\n",
    "train_wrangled['Title'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e5d3ba8-0e5a-4050-9a85-64c7a1c49773",
   "metadata": {},
   "source": [
    "## Part 2 - Model Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3b9487-a14e-4cb9-a093-dbd0bc89cb7e",
   "metadata": {},
   "source": [
    "### Step 10 - Feature Cleanup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd9734-3fa6-4628-86bc-2b8e04de259b",
   "metadata": {},
   "source": [
    "A common approach to addressing long-tail categorical values is to replace them with a designated category (e.g., *Other*). The following code performs this data wrangling on the *Title* feature long-tail values. Type the following code into the blank code cell in your lab notebook and run it to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6b40b6-8e1b-421e-80c3-5fc9237def76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a088807d-f66e-46c3-8c18-af3da0e82754",
   "metadata": {},
   "source": [
    "### Step 11 - Rebuild Predictors DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbf91323-ec3f-4a8d-9719-97ee8e94dc7d",
   "metadata": {},
   "source": [
    "Given the data wrangling of Step 10, you need to rebuild the predictors DataFrame to use the new *Other* category. Run the code cell below to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26ee3cc-0daf-4932-95ab-3fe822001a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Relearn the encodings and transform data\n",
    "train_cat = cat_encoder.fit_transform(train_wrangled[cat_features])\n",
    "train_cat.head()\n",
    "\n",
    "# Rebuild the predictors DataFrame\n",
    "titanic_X = pd.concat([train_wrangled[num_features], train_cat], axis = 1)\n",
    "titanic_X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14e3219-e3eb-4b7d-a040-fafc633b032e",
   "metadata": {},
   "source": [
    "### Step 12 - Train a Tuned Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e10f99-1586-431d-91c8-0e9cb232705c",
   "metadata": {},
   "source": [
    "As discussed in the lecture, random forests' use of *bagging* is analogous to *cross-validation*. While cross-validation can be used to tune random forests, relying on the *out-of-bag (OOB)* estimates is far more efficient. Type the following code into the blank code cell in your lab notebook and run it to produce the results.\n",
    "\n",
    "**NOTE -** The following code takes a while to run. If you can, increasing the value of the *n_job* parameter will reduce the execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "923405c3-fc66-4f71-ab75-16824f79b97a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type you lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8d4127-33f3-4813-9394-e77155cce028",
   "metadata": {},
   "source": [
    "### Step 13 - Get the Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23196406-a9b4-47cb-bd74-822187c8d079",
   "metadata": {},
   "source": [
    "Type the following code into the blank code cell in your lab notebook and run it to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2585a4da-68c6-4dd9-95f9-3f7b640c2387",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad9a713-9334-420f-8b15-6321e6689808",
   "metadata": {},
   "source": [
    "### Step 14 - Evaluating Bias and Variance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a0a2b8-f84f-45a4-b60a-3a833c90df9a",
   "metadata": {},
   "source": [
    "Unlike single decision trees, you can't realistically understand random forests using visualizations. While it's possible to visualize individual trees in the random forest, understanding how 100s of trees work togther is just too difficult. In practice, we rely on techniques like *permutation importance*, confusion matrices, and evaluating bias and variance to develop an understanding of random forest models. Run the following code cell to produce the results.\n",
    "\n",
    "Note the following in the output:\n",
    "* The mean accuracy (i.e., *bias*) is approximately 82.5% accuracy across the 100 models\n",
    "* The standard deviation (i.e., *variance*) is approximately 0.3% across the 100 models\n",
    "\n",
    "To interpret the variance:\n",
    "* We would expect the model to score between 82.3% and 82.9% accuracy about 2/3 of the time.\n",
    "* We would expect the model to score between 82.0% and 83.2% accuracy about 95% of the time.\n",
    "\n",
    "**Bottom Line -** The random forest has improved accuracy compared to the decision tree from Lab #4 and also has far, far lower variance!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d328503c-2591-418c-b962-5d9913cececd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab the best hyperparameter values\n",
    "params = param_list[best_params]\n",
    "\n",
    "# Train 100 models using the best hyperparameter values\n",
    "oob_accuracy = []\n",
    "for value in range(0, 100):\n",
    "    # Do NOT set random_state!\n",
    "    rf = RandomForestClassifier(n_estimators = params['n_estimators'], max_features = params['max_features'], \n",
    "                                min_samples_leaf = params['min_samples_leaf'], oob_score = True, n_jobs = 2)\n",
    "    rf.fit(titanic_X, titanic_train['Survived'])\n",
    "    oob_accuracy.append(rf.oob_score_)\n",
    "\n",
    "# Evaluate the bias and variance across the 100 models\n",
    "# Due to randomness, you will likely see slightly different results\n",
    "print(f'Mean OOB Accuracy: {np.average(oob_accuracy)}')\n",
    "print(f'OOB Accuracy Std Deviation: {np.std(oob_accuracy)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acddb0b4-9a20-4f82-81fb-6a87ab5d8f4d",
   "metadata": {},
   "source": [
    "## Part 3 - Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d72c35d3-d7bd-4531-bc8a-b9debb0795f3",
   "metadata": {},
   "source": [
    "### Step 15 - Preparing the Test Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5e1aa5-9126-4647-a219-1fe040afe35a",
   "metadata": {},
   "source": [
    "Before predictions can be made on the test dataset, it needs to be wrangled using the same logic that created the predictors DataFrame. Examine the code cell output to ensure the *test_X* DataFrame has the same columns as the *titanic_X* DataFrame. Also, be sure to check for any missing values. Run the following code cell to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111d8de3-77f7-4789-b9c9-841a532e16d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load Titanic test data from CSV file\n",
    "titanic_test = pd.read_csv('titanic_test.csv')\n",
    "\n",
    "# Create the test_wrangled DataFrame\n",
    "test_wrangled = (titanic_test\n",
    "                   .assign(Female = lambda df_: df_['Sex'].replace({'female': 1, 'male': 0}),\n",
    "                           PartySize = lambda df_: df_['SibSp'] + df_['Parch'] + 1,\n",
    "                           PartyFare = lambda df_: df_['Fare'] / df_['PartySize'],\n",
    "                           Embarked = lambda df_: df_['Embarked'].fillna('S'),\n",
    "                           CommaSplit = lambda df_: df_['Name'].str.split(', ', expand = True).loc[:, 1],\n",
    "                           Title = lambda df_: df_['CommaSplit'].str.split('.', expand = True).loc[:, 0])\n",
    "                 )\n",
    "\n",
    "\n",
    "# Overwrite unimportant titles with 'Other'\n",
    "test_title_mask = test_wrangled['Title'].isin(important_titles)\n",
    "test_wrangled.loc[~test_title_mask, 'Title'] = 'Other'\n",
    "\n",
    "# Reuse the OneHotEncoder object to ensure the test dataset matches the train dataset!\n",
    "test_cat = cat_encoder.transform(test_wrangled[cat_features])\n",
    "\n",
    "# Build the wrangled test dataset\n",
    "test_X = pd.concat([test_wrangled[num_features], test_cat], axis = 1)\n",
    "test_X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d840d3-3476-4d6b-94d3-d241b7e6e671",
   "metadata": {},
   "source": [
    "### Step 16 - Impute Missing Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3fdde2-83f8-43cf-9eec-c524293eb333",
   "metadata": {},
   "source": [
    "The output of Step 15 shows that a single value is missing for the *PartyFare* feature. Given that *PartyFare* has skewed values, a reasonable imputation approach is to replace the single missing value with the median of *PartyFare*. Type the following code into the blank code cell in your lab notebook and run it to produce the results.\n",
    "\n",
    "**NOTE -** Imputation is *always* performed using the training data applied to the test data to avoid information leakage!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f6bf7e0-3bcd-47b4-a836-0e0681449799",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cf98ad-a735-4db1-93d4-8edb4b17e76f",
   "metadata": {},
   "source": [
    "### Step 17 - Make Predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d98aa2-ffdf-40bb-932a-7f3751bf46ab",
   "metadata": {},
   "source": [
    "To make predictions for the test set, one final *RandomForestClassifier* must be trained using the optimal hyperparameter vaues. Type the following code into the blank code cell in your lab notebook and run it to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2714f36b-d159-434a-ae37-34b9288cd6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Type your lab code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59774abd-1bd8-44c7-9397-81787c89c20c",
   "metadata": {},
   "source": [
    "### Step 18 - Save Predictions to .CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b432323d-5807-45a6-b8ac-f94efc2af794",
   "metadata": {},
   "source": [
    "If you're interested, the following code creates a .CSV file suitable for submission to the [**Kaggle Titanic competition**](https://www.kaggle.com/competitions/titanic). Run the following code cell to produce the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3142cdd3-780c-439d-a31e-7b5adc857b3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a Survived column to the Titanic test dataset\n",
    "titanic_test['Survived'] = test_predictions\n",
    "\n",
    "# Save Kaggle submission .CSV\n",
    "titanic_test[['PassengerId', 'Survived']].to_csv('TitanicSubmission.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
